---
layout: archive
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi, I am Jifeng Song, a third-year Ph.D. student in Electrical and Computer Engineering at the [University of Pittsburgh](https://www.pitt.edu/) and also a research assistant in the Cancer Virology Program at [UPMC Hillman Cancer Center](https://hillman.upmc.com/), co-advised by [Prof. Yufei Huang](https://www.sci.pitt.edu/people/yufei-huang) and [Prof. Zhi-Hong Mao](https://sites.pitt.edu/~zhm4/). My current research focuses on <b>Multimodal Learning</b> and <b>AI for Biomedicine</b>, with broader interests in AI for scientific discovery, computational biology, explainable AI, and efficient large language models. I received my B.E. in Electrical Engineering and Automation from [Huazhong University of Science and Technology](https://www.hust.edu.cn/), where I worked on transfer learning for renewable energy forecasting.

Here is my [CV](http://Muhusystem.github.io/files/CV_JifengSong.pdf). My current work focuses on <b>vision-language models for biomedical applications</b>. I am training biomedical multimodal large models for cancer research, particularly in spatial transcriptomics and single-cell, including multimodal retrieval, image captioning, question answering, and hypothesis generation.


## ðŸ“– Education

<b>Ph.D. student</b>, Electrical and Computer Engineering, University of Pittsburgh, Sept. 2023 - Present

<b>M.S.</b>, Electrical and Computer Engineering, University of Pittsburgh, Sept. 2023 - August 2025 &#124; GPA: 3.83/4

<b>B.E.</b>, Electrical Engineering, Huazhong University of Science and Technology, Sept. 2019 - July 2023 &#124; GPA: 3.77/4

## News

<div class="news-section">
* <span class="news-date">[01/2025]</span> Our paper <strong>FigEx2</strong> is released on arXiv. <a href="https://arxiv.org/abs/2601.08026">Check it out!</a>
* <span class="news-date">[12/2024]</span> Our paper <strong>FigEx</strong> is accepted by EMNLP 2025 Findings. Congrats to the team!
* <span class="news-date">[12/2024]</span> Our paper <strong>Aligning Findings with Diagnosis</strong> is released on arXiv.
* <span class="news-date">[11/2024]</span> Our survey paper <strong>A Process-Centric Survey of AI for Scientific Discovery</strong> is released on Research Square.
* <span class="news-date">[06/2024]</span> Our paper <strong>Achieving Sparse Activation in Small Language Models</strong> is released on arXiv.
</div>

## Selected Publications | [Full list]({{ base_path }}/publications/)

<div class="selected-publication">
  <div class="pub-thumbnail">
    <img src="{{ base_path }}/images/figex.png" alt="FigEx">
  </div>
  <div class="pub-content">
    <div class="pub-badge badge-emnlp">EMNLP 2025 Findings</div>
    <div class="pub-title">FigEx: Aligned Extraction of Scientific Figures and Captions</div>
    <div class="pub-authors">
      <strong><span class="author-me">Jifeng Song</span></strong>, <strong>Arun Das</strong>, <strong>Ge Cui</strong>, <strong>Yufei Huang</strong>
    </div>
    <div class="pub-venue">Findings of the Association for Computational Linguistics: EMNLP 2025</div>
    <div class="pub-links">
      <a href="https://aclanthology.org/2025.findings-emnlp.899/">paper</a>
      <a href="https://github.com/Huang-AI4Medicine-Lab/FigEx">code</a>
    </div>
    <div class="pub-abstract">A framework for aligned extraction of scientific figures and their captions from research papers.</div>
  </div>
</div>

<div class="selected-publication">
  <div class="pub-thumbnail">
    <img src="{{ base_path }}/images/500x300.png" alt="FigEx2">
  </div>
  <div class="pub-content">
    <div class="pub-badge badge-arxiv">arXiv</div>
    <div class="pub-title">FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures</div>
    <div class="pub-authors">
      <strong><span class="author-me">Jifeng Song</span></strong>, <strong>Arun Das</strong>, <strong>Pan Wang</strong>, <strong>Hui Ji</strong>, <strong>Kun Zhao</strong>, <strong>Yufei Huang</strong>
    </div>
    <div class="pub-venue">arXiv preprint 2025</div>
    <div class="pub-links">
      <a href="https://arxiv.org/abs/2601.08026">paper</a>
    </div>
    <div class="pub-abstract">A visual-conditioned approach for detecting and captioning panels in scientific compound figures.</div>
  </div>
</div>

<div class="selected-publication">
  <div class="pub-thumbnail">
    <img src="{{ base_path }}/images/500x300.png" alt="Radiology Reporting">
  </div>
  <div class="pub-content">
    <div class="pub-badge badge-arxiv">arXiv</div>
    <div class="pub-title">Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting</div>
    <div class="pub-authors">
      <strong>Kun Zhao</strong>, <strong>Siyuan Dai</strong>, <strong>Pan Wang</strong>, <strong><span class="author-me">Jifeng Song</span></strong>, <strong>Hui Ji</strong>, <strong>Chenghua Lin</strong>, <strong>Liang Zhan</strong>, <strong>Haoteng Tang</strong>
    </div>
    <div class="pub-venue">arXiv preprint 2025</div>
    <div class="pub-links">
      <a href="https://arxiv.org/abs/2601.03321">paper</a>
    </div>
    <div class="pub-abstract">A self-consistent reinforcement learning framework for generating trustworthy radiology reports that align findings with diagnosis.</div>
  </div>
</div>

<div class="selected-publication">
  <div class="pub-thumbnail">
    <img src="{{ base_path }}/images/500x300.png" alt="EXHYTE Framework">
  </div>
  <div class="pub-content">
    <div class="pub-badge badge-research-square">Res Sq</div>
    <div class="pub-title">A Process-Centric Survey of AI for Scientific Discovery Through the EXHYTE Framework</div>
    <div class="pub-authors">
      <strong>Md Musaddaqul Hasib</strong>, <strong>Sumin Jo</strong>, <strong>Harsh Sinha</strong>, <strong><span class="author-me">Jifeng Song</span></strong>, <strong>Arun Das</strong>, <strong>Zhentao Liu</strong>, <strong>Hugh Galloway</strong>, <strong>Huey Huang</strong>, <strong>Kexun Zhang</strong>, <strong>Shou-Jiang Gao</strong>, <strong>Yu-Chiao Chiu</strong>, <strong>Lei Li</strong>, <strong>Yufei Huang</strong>
    </div>
    <div class="pub-venue">Research Square preprint 2024</div>
    <div class="pub-links">
      <a href="https://doi.org/10.21203/rs.3.rs-8370059/v1">paper</a>
    </div>
    <div class="pub-abstract">A comprehensive survey of AI applications in scientific discovery organized through the EXHYTE framework.</div>
  </div>
</div>

<div class="selected-publication">
  <div class="pub-thumbnail">
    <img src="{{ base_path }}/images/500x300.png" alt="Sparse Activation">
  </div>
  <div class="pub-content">
    <div class="pub-badge badge-other">arXiv</div>
    <div class="pub-title">Achieving Sparse Activation in Small Language Models</div>
    <div class="pub-authors">
      <strong><span class="author-me">Jifeng Song</span></strong>, <strong>Kai Huang</strong>, <strong>Xiangyu Yin</strong>, <strong>Boyuan Yang</strong>, <strong>Wei Gao</strong>
    </div>
    <div class="pub-venue">arXiv preprint 2024</div>
    <div class="pub-links">
      <a href="https://arxiv.org/abs/2406.06562">paper</a>
      <a href="https://github.com/pittisl/Sparse-Activation">code</a>
    </div>
    <div class="pub-abstract">A method for achieving sparse activation patterns in small language models to improve efficiency and interpretability.</div>
  </div>
</div>


